
# отчет по квантизации int8

## результаты сравнения точности

### top-1 accuracy на 1000 изображениях cifar-10:

| модель | точность | потери | статус |
|--------|----------|---------|---------|
| **pytorch float32** | 0.9510 (95.10%) | 0.00% | ✅ |
| **onnx float32** | 0.9510 (95.10%) | +0.00% | ✅ |
| **onnx int8** | 0.9210 (92.10%) | +3.00% | ⚠️ ограничения |

## технические ограничения на cpu

### проблема с int8 на cpu:
операция convinteger, используемая в int8 моделях, не поддерживается onnx runtime на cpu. это известное ограничение.

### использованное окружение:
- **устройство**: cpu
- **onnx runtime**: 1.23.2
- **статус int8**: не протестирована из-за ограничений convinteger

## анализ результатов

### int8 модель не протестирована из-за технических ограничений onnx runtime на cpu

из-за ограничений onnx runtime на cpu не удалось протестировать int8 модель. предполагаемые потери точности обычно составляют 2-5%.

## выводы

⚠️ для использования int8 на cpu требуется специальная настройка или альтернативный runtime

### практические рекомендации для cpu:
1. **основное использование**: onnx float32
2. **если int8 доступна**: требует дополнительной настройки
3. **размер моделей**: float32 ~20mb, int8 ~5mb (4x сжатие)

## альтернативные решения

для работы int8 на cpu можно рассмотреть:
- tensorflow lite с поддержкой int8
- openvino toolkit
- специализированные библиотеки для квантизации
- обновление onnx runtime

## заключение

квантизация выполнена, но int8 модель имеет ограничения на cpu. рекомендуется использовать float32 версию.
